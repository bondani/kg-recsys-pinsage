{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312261ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.legacy import data\n",
    "import dgl\n",
    "import tqdm\n",
    "\n",
    "import layers\n",
    "import sampler as sampler_module\n",
    "import evaluation\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e87ed5",
   "metadata": {},
   "source": [
    "Добавление механизма внимания вместо весов, полученных на основе случайного блуждания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eaadb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        self.Q = nn.Linear(input_dims, hidden_dims)\n",
    "        self.ATTN = nn.Linear(2 * hidden_dims, 1)\n",
    "        self.W = nn.Linear(input_dims + hidden_dims, output_dims)\n",
    "        self.reset_parameters()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_uniform_(self.Q.weight, gain=gain)\n",
    "        nn.init.xavier_uniform_(self.W.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.ATTN.weight, gain=gain)\n",
    "        nn.init.constant_(self.Q.bias, 0)\n",
    "        nn.init.constant_(self.W.bias, 0)\n",
    "        nn.init.constant_(self.ATTN.bias, 0)\n",
    "        \n",
    "    def attention_score(self, edges):\n",
    "        a = self.ATTN(torch.cat([edges.src['n'], edges.dst['n']], dim=1))\n",
    "        return {'e': F.leaky_relu(a)}\n",
    "    \n",
    "    def message_func(self, edges):\n",
    "        return {'n': edges.src['n'], 'e': edges.data['e']}\n",
    "    \n",
    "    def reduce_func(self, nodes):\n",
    "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "        h = torch.sum(alpha * nodes.mailbox['n'], dim=1)\n",
    "        return {'h': h}\n",
    "    \n",
    "    def forward(self, block, h):\n",
    "        h_src, h_dst = h\n",
    "        with block.local_scope():\n",
    "            # add n_u\n",
    "            block.srcdata['n'] = self.Q(self.dropout(h_src))\n",
    "            block.dstdata['n'] = block.srcdata['n'][:len(block.dstdata[dgl.NID])]\n",
    "            \n",
    "            # add aggregation (attention) n_u\n",
    "            block.apply_edges(self.attention_score)\n",
    "\n",
    "            block.update_all(self.message_func, self.reduce_func)\n",
    "            \n",
    "            z = F.relu(self.W(self.dropout(torch.cat([block.dstdata['h'], h_dst], 1))))\n",
    "            z_norm = z.norm(2, 1, keepdim=True)\n",
    "            z_norm = torch.where(z_norm == 0, torch.tensor(1.).to(z_norm), z_norm)\n",
    "            z = z / z_norm\n",
    "            return z\n",
    "        \n",
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, num_heads, merge='cat'):\n",
    "        super(MultiHeadAttentionLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(AttentionLayer(input_dims, hidden_dims, output_dims))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, block, h):\n",
    "        z_items = []\n",
    "        for head in self.heads:\n",
    "            z_item = head(block, h)\n",
    "            z_items.append(z_item)\n",
    "    \n",
    "        if self.merge == 'cat':\n",
    "            return torch.cat(z_items, dim=1)\n",
    "        else:\n",
    "            return torch.mean(torch.stack(z_items), 0)\n",
    "       \n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dims, num_heads, num_layers):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.convs.append(MultiHeadAttentionLayer(hidden_dims, hidden_dims, hidden_dims, num_heads=2, merge='mean'))\n",
    "\n",
    "    def forward(self, blocks, h):\n",
    "        for layer, block in zip(self.convs, blocks):\n",
    "            h_dst = h[:block.number_of_nodes('DST/' + block.ntypes[0])]\n",
    "            h = layer(block, (h, h_dst))     \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adace7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinSAGEModel(nn.Module):\n",
    "    def __init__(self, full_graph, ntype, textsets, hidden_dims, n_layers, n_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.proj = layers.LinearProjector(full_graph, ntype, textsets, hidden_dims)\n",
    "        self.net = Net(hidden_dims, n_heads, n_layers)\n",
    "        self.scorer = layers.ItemToItemScorer(full_graph, ntype)\n",
    "\n",
    "    def forward(self, pos_graph, neg_graph, blocks):\n",
    "        h_item = self.get_repr(blocks)\n",
    "        pos_score = self.scorer(pos_graph, h_item)\n",
    "        neg_score = self.scorer(neg_graph, h_item)\n",
    "        return (neg_score - pos_score + 1).clamp(min=0)\n",
    "\n",
    "    def get_repr(self, blocks):\n",
    "        h_item = self.proj(blocks[0].srcdata)\n",
    "        h_item_dst = self.proj(blocks[-1].dstdata)\n",
    "        return h_item_dst + self.net(blocks, h_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40f700b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainArgs:\n",
    "    output_model_path: str\n",
    "    random_walk_length: int = 2\n",
    "    random_walk_restart_prob: float = 0.5\n",
    "    num_random_walks: int = 10\n",
    "    num_neighbors: int = 5\n",
    "    num_layers: int = 2\n",
    "    num_heads: int = 2\n",
    "    hidden_dims: int = 16\n",
    "    batch_size: int = 64\n",
    "    device: str = 'cpu'\n",
    "    num_epochs: int = 1\n",
    "    batches_per_epoch: int = 20000\n",
    "    num_workers: int = 0\n",
    "    lr: float = 3e-5\n",
    "    k: int = 10\n",
    "    n_latest_items: int = 10\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb42e9",
   "metadata": {},
   "source": [
    "ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e04fb4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/data_ml.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    \n",
    "args = TrainArgs(output_model_path='models_attention/model_ml_optuna', num_epochs=5, \n",
    "                 hidden_dims=64, batches_per_epoch=10000, device='cuda', k=20)\n",
    "\n",
    "\n",
    "g = dataset['train-graph']\n",
    "val_matrix = dataset['val-matrix'].tocsr()\n",
    "test_matrix = dataset['test-matrix'].tocsr()\n",
    "item_texts = dataset['item-texts']\n",
    "user_ntype = dataset['user-type']\n",
    "item_ntype = dataset['item-type']\n",
    "user_to_item_etype = dataset['user-to-item-type']\n",
    "timestamp = dataset['timestamp-edge-column']\n",
    "device = torch.device(args.device)\n",
    "# Assign user and movie IDs and use them as features (to learn an individual trainable\n",
    "# embedding for each entity)\n",
    "g.nodes[user_ntype].data['id'] = torch.arange(g.number_of_nodes(user_ntype))\n",
    "g.nodes[item_ntype].data['id'] = torch.arange(g.number_of_nodes(item_ntype))\n",
    "# Prepare torchtext dataset and vocabulary\n",
    "if item_texts is not None:\n",
    "    fields = {}\n",
    "    examples = []\n",
    "    for key, texts in item_texts.items():\n",
    "        fields[key] = data.Field(include_lengths=True, lower=True, batch_first=True)\n",
    "    for i in range(g.number_of_nodes(item_ntype)):\n",
    "        example = data.Example.fromlist(\n",
    "            [item_texts[key][i] for key in item_texts.keys()],\n",
    "            [(key, fields[key]) for key in item_texts.keys()])\n",
    "        examples.append(example)\n",
    "    textset = data.Dataset(examples, fields)\n",
    "    for key, field in fields.items():\n",
    "        field.build_vocab(getattr(textset, key))\n",
    "        #field.build_vocab(getattr(textset, key), vectors='fasttext.simple.300d')\n",
    "else:\n",
    "    textset = None\n",
    "# Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ea00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-29 15:00:47,293]\u001b[0m A new study created in memory with name: no-name-f634da57-13b1-45fa-9715-5cae64abaeba\u001b[0m\n",
      " 27%|██████████████████████████████████▊                                                                                               | 2677/10000 [06:33<26:54,  4.54it/s]"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    n_heads = trial.suggest_int('n_heads', 1, 3)\n",
    "    hidden_dims = trial.suggest_int('hidden_dims', 32, 128)\n",
    "    #num_epochs = trial.suggest_int('num_epochs', 3, 10)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate_init\", 1e-5, 1e-3)\n",
    "    num_neighbors = trial.suggest_int('num_neighbors', 1, 15)\n",
    "    \n",
    "    batch_sampler = sampler_module.ItemToItemBatchSampler(\n",
    "        g, user_ntype, item_ntype, args.batch_size)\n",
    "    neighbor_sampler = sampler_module.NeighborSampler(\n",
    "        g, user_ntype, item_ntype, args.random_walk_length,\n",
    "        args.random_walk_restart_prob, args.num_random_walks, num_neighbors,\n",
    "        n_layers)\n",
    "    collator = sampler_module.PinSAGECollator(neighbor_sampler, g, item_ntype, textset)\n",
    "    dataloader = DataLoader(\n",
    "        batch_sampler,\n",
    "        collate_fn=collator.collate_train,\n",
    "        num_workers=args.num_workers)\n",
    "    dataloader_test = DataLoader(\n",
    "        torch.arange(g.number_of_nodes(item_ntype)),\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=collator.collate_test,\n",
    "        num_workers=args.num_workers)\n",
    "    dataloader_it = iter(dataloader)\n",
    "    \n",
    "    model = PinSAGEModel(g, item_ntype, textset, hidden_dims, n_layers, n_heads).to(args.device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    layers = []\n",
    "\n",
    "    for epoch_id in range(2):\n",
    "        model.train()\n",
    "        for batch_id in tqdm.trange(args.batches_per_epoch):\n",
    "            pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "            # Copy to GPU\n",
    "            for i in range(len(blocks)):\n",
    "                blocks[i] = blocks[i].to(device)\n",
    "            pos_graph = pos_graph.to(device)\n",
    "            neg_graph = neg_graph.to(device)\n",
    "\n",
    "            loss = model(pos_graph, neg_graph, blocks).mean()\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            item_batches = torch.arange(g.number_of_nodes(item_ntype)).split(args.batch_size)\n",
    "            h_item_batches = []\n",
    "            for blocks in dataloader_test:\n",
    "                for i in range(len(blocks)):\n",
    "                    blocks[i] = blocks[i].to(device)\n",
    "\n",
    "                h_item_batches.append(model.get_repr(blocks))\n",
    "            h_item = torch.cat(h_item_batches, 0)\n",
    "            metrics = evaluation.evaluate_nn(dataset, h_item, args.k, args.batch_size, args.n_latest_items)\n",
    "            \n",
    "    return metrics[0][2]\n",
    "\n",
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1, timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb50b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainArgs(output_model_path='models_attention/model_ml_optuna', num_epochs=5, \n",
    "                 hidden_dims=69, batches_per_epoch=10000, device='cuda', k=20,\n",
    "                 lr = 3.65e-05, num_neighbors = 9, num_heads=3, num_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23786a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [10:26<00:00, 15.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.07684602649006848, 0.06444941453818276, 0.22953112314388738) \n",
      " Rec by N latest items (0.08195364238410799, 0.06713630699920319, 0.23830760938249615)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [10:43<00:00, 15.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.10547185430463589, 0.09508574268523141, 0.3649909037398669) \n",
      " Rec by N latest items (0.11462748344370854, 0.10300242999089733, 0.37943280894274156)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [10:28<00:00, 15.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.1127649006622514, 0.10242504343950844, 0.4025777391102182) \n",
      " Rec by N latest items (0.11887417218543024, 0.10662430712087431, 0.3996387766863449)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [10:25<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "RESULT\n",
      "Epoch 3, Rec by latest item: PR@1: 0.15281456953642383|REC@1: 0.007639486679528726|NDCG@1: 0.02384105960264901\n",
      "Epoch 3, Rec by 10 latest items: PR@1: 0.16026490066225166|REC@1: 0.007520462346761501|NDCG@1: 0.02185430463576159\n",
      "Epoch 3, Rec by latest item: PR@5: 0.14172185430463838|REC@5: 0.0345450778146431|NDCG@5: 0.14461230349135962\n",
      "Epoch 3, Rec by 10 latest items: PR@5: 0.14450331125828073|REC@5: 0.03417358236809236|NDCG@5: 0.1509793804480531\n",
      "Epoch 3, Rec by latest item: PR@10: 0.13064569536424264|REC@10: 0.06187579650079092|NDCG@10: 0.25798118762065086\n",
      "Epoch 3, Rec by 10 latest items: PR@10: 0.13369205298013645|REC@10: 0.062318556255081245|NDCG@10: 0.2681922757820343\n",
      "Epoch 3, Rec by latest item: PR@15: 0.12113686534216622|REC@15: 0.08356225039798802|NDCG@15: 0.3426607270048519\n",
      "Epoch 3, Rec by 10 latest items: PR@15: 0.1263024282560739|REC@15: 0.08598403448166919|NDCG@15: 0.3409933649465797\n",
      "Epoch 3, Rec by latest item: PR@20: 0.11502483443708561|REC@20: 0.10374627328767387|NDCG@20: 0.4001150355539444\n",
      "Epoch 3, Rec by 10 latest items: PR@20: 0.12086092715231758|REC@20: 0.10732740235190033|NDCG@20: 0.4021292090888023\n"
     ]
    }
   ],
   "source": [
    "batch_sampler = sampler_module.ItemToItemBatchSampler(\n",
    "    g, user_ntype, item_ntype, args.batch_size)\n",
    "neighbor_sampler = sampler_module.NeighborSampler(\n",
    "    g, user_ntype, item_ntype, args.random_walk_length,\n",
    "    args.random_walk_restart_prob, args.num_random_walks, args.num_neighbors,\n",
    "    args.num_layers)\n",
    "collator = sampler_module.PinSAGECollator(neighbor_sampler, g, item_ntype, textset)\n",
    "dataloader = DataLoader(\n",
    "    batch_sampler,\n",
    "    collate_fn=collator.collate_train,\n",
    "    num_workers=args.num_workers)\n",
    "dataloader_test = DataLoader(\n",
    "    torch.arange(g.number_of_nodes(item_ntype)),\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=collator.collate_test,\n",
    "    num_workers=args.num_workers)\n",
    "dataloader_it = iter(dataloader)\n",
    "# Model\n",
    "model = PinSAGEModel(g, item_ntype, textset, args.hidden_dims, args.num_layers, args.num_heads).to(device)\n",
    "# Optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "base_ndcg = 0\n",
    "# For each batch of head-tail-negative triplets...\n",
    "for epoch_id in range(args.num_epochs):\n",
    "    model.train()\n",
    "    for batch_id in tqdm.trange(args.batches_per_epoch):\n",
    "        pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "        # Copy to GPU\n",
    "        for i in range(len(blocks)):\n",
    "            blocks[i] = blocks[i].to(device)\n",
    "        pos_graph = pos_graph.to(device)\n",
    "        neg_graph = neg_graph.to(device)\n",
    "        loss = model(pos_graph, neg_graph, blocks).mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    #with open(args.output_model_path, 'wb') as f:\n",
    "        #pickle.dump(model, f)\n",
    "        \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        item_batches = torch.arange(g.number_of_nodes(item_ntype)).split(args.batch_size)\n",
    "        h_item_batches = []\n",
    "        for blocks in dataloader_test:\n",
    "            for i in range(len(blocks)):\n",
    "                blocks[i] = blocks[i].to(device)\n",
    "            h_item_batches.append(model.get_repr(blocks))\n",
    "        h_item = torch.cat(h_item_batches, 0)\n",
    "        metrics = evaluation.evaluate_nn(dataset, h_item, args.k, args.batch_size, args.n_latest_items)\n",
    "        if metrics[0][2] - base_ndcg > -0.001:\n",
    "            old_metrics = metrics\n",
    "            base_ndcg = metrics[0][2]\n",
    "            torch.save(model.state_dict(), args.output_model_path)\n",
    "        \n",
    "            print('Rec by latest item', metrics[0],\n",
    "                  '\\n',\n",
    "                  'Rec by N latest items', metrics[1])\n",
    "        else:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "print('RESULT')\n",
    "for k in [1, 5, 10, 15, 20]:\n",
    "    metrics = evaluation.evaluate_nn(dataset, h_item, k, args.batch_size, args.n_latest_items)\n",
    "    \n",
    "    print(f'Epoch {epoch_id}, Rec by latest item: PR@{k}: {metrics[0][0]}|REC@{k}: {metrics[0][1]}|NDCG@{k}: {metrics[0][2]}')\n",
    "    print(f'Epoch {epoch_id}, Rec by {args.n_latest_items} latest items: PR@{k}: {metrics[1][0]}|REC@{k}: {metrics[1][1]}|NDCG@{k}: {metrics[1][2]}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53764d71",
   "metadata": {},
   "source": [
    "Ta Feng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "601e3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tafeng.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    \n",
    "args = TrainArgs(output_model_path='models_attention/model_tafeng_optuna', num_epochs=5, \n",
    "                 hidden_dims=64, batches_per_epoch=10000, device='cuda', k=20)\n",
    "\n",
    "\n",
    "g = dataset['train-graph']\n",
    "val_matrix = dataset['val-matrix'].tocsr()\n",
    "test_matrix = dataset['test-matrix'].tocsr()\n",
    "item_texts = dataset['item-texts']\n",
    "user_ntype = dataset['user-type']\n",
    "item_ntype = dataset['item-type']\n",
    "user_to_item_etype = dataset['user-to-item-type']\n",
    "timestamp = dataset['timestamp-edge-column']\n",
    "device = torch.device(args.device)\n",
    "# Assign user and movie IDs and use them as features (to learn an individual trainable\n",
    "# embedding for each entity)\n",
    "g.nodes[user_ntype].data['id'] = torch.arange(g.number_of_nodes(user_ntype))\n",
    "g.nodes[item_ntype].data['id'] = torch.arange(g.number_of_nodes(item_ntype))\n",
    "# Prepare torchtext dataset and vocabulary\n",
    "if item_texts is not None:\n",
    "    fields = {}\n",
    "    examples = []\n",
    "    for key, texts in item_texts.items():\n",
    "        fields[key] = data.Field(include_lengths=True, lower=True, batch_first=True)\n",
    "    for i in range(g.number_of_nodes(item_ntype)):\n",
    "        example = data.Example.fromlist(\n",
    "            [item_texts[key][i] for key in item_texts.keys()],\n",
    "            [(key, fields[key]) for key in item_texts.keys()])\n",
    "        examples.append(example)\n",
    "    textset = data.Dataset(examples, fields)\n",
    "    for key, field in fields.items():\n",
    "        field.build_vocab(getattr(textset, key))\n",
    "        #field.build_vocab(getattr(textset, key), vectors='fasttext.simple.300d')\n",
    "else:\n",
    "    textset = None\n",
    "# Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eae1f8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-27 12:56:04,733]\u001b[0m A new study created in memory with name: no-name-d3513a46-090e-4017-91e6-59cb90c2a72b\u001b[0m\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [08:57<00:00, 18.61it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [08:54<00:00, 18.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [08:58<00:00, 18.58it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [08:57<00:00, 18.60it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [08:59<00:00, 18.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [08:59<00:00, 18.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [08:59<00:00, 18.53it/s]\n",
      "\u001b[32m[I 2022-05-27 14:14:23,634]\u001b[0m Trial 0 finished with value: 0.014192362365067998 and parameters: {'n_layers': 3, 'n_heads': 3, 'hidden_dims': 41, 'num_epochs': 7, 'learning_rate_init': 0.0001705221683931775, 'num_neighbors': 3}. Best is trial 0 with value: 0.014192362365067998.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    n_heads = trial.suggest_int('n_heads', 1, 3)\n",
    "    hidden_dims = trial.suggest_int('hidden_dims', 16, 128)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 3, 10)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate_init\", 1e-5, 1e-3)\n",
    "    num_neighbors = trial.suggest_int('num_neighbors', 1, 15)\n",
    "    \n",
    "    batch_sampler = sampler_module.ItemToItemBatchSampler(\n",
    "        g, user_ntype, item_ntype, args.batch_size)\n",
    "    neighbor_sampler = sampler_module.NeighborSampler(\n",
    "        g, user_ntype, item_ntype, args.random_walk_length,\n",
    "        args.random_walk_restart_prob, args.num_random_walks, num_neighbors,\n",
    "        n_layers)\n",
    "    collator = sampler_module.PinSAGECollator(neighbor_sampler, g, item_ntype, textset)\n",
    "    dataloader = DataLoader(\n",
    "        batch_sampler,\n",
    "        collate_fn=collator.collate_train,\n",
    "        num_workers=args.num_workers)\n",
    "    dataloader_test = DataLoader(\n",
    "        torch.arange(g.number_of_nodes(item_ntype)),\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=collator.collate_test,\n",
    "        num_workers=args.num_workers)\n",
    "    dataloader_it = iter(dataloader)\n",
    "    \n",
    "    model = PinSAGEModel(g, item_ntype, textset, hidden_dims, n_layers, n_heads).to(args.device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    layers = []\n",
    "\n",
    "    for epoch_id in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_id in tqdm.trange(args.batches_per_epoch):\n",
    "            pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "            # Copy to GPU\n",
    "            for i in range(len(blocks)):\n",
    "                blocks[i] = blocks[i].to(device)\n",
    "            pos_graph = pos_graph.to(device)\n",
    "            neg_graph = neg_graph.to(device)\n",
    "\n",
    "            loss = model(pos_graph, neg_graph, blocks).mean()\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            item_batches = torch.arange(g.number_of_nodes(item_ntype)).split(args.batch_size)\n",
    "            h_item_batches = []\n",
    "            for blocks in dataloader_test:\n",
    "                for i in range(len(blocks)):\n",
    "                    blocks[i] = blocks[i].to(device)\n",
    "\n",
    "                h_item_batches.append(model.get_repr(blocks))\n",
    "            h_item = torch.cat(h_item_batches, 0)\n",
    "            metrics = evaluation.evaluate_nn(dataset, h_item, args.k, args.batch_size, args.n_latest_items)\n",
    "            \n",
    "    return metrics[0][2]\n",
    "\n",
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1, timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07030dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainArgs(output_model_path='models_attention/model_tafeng_optuna', num_epochs=7, \n",
    "                 hidden_dims=41, batches_per_epoch=10000, device='cuda', k=20,\n",
    "                 lr = 0.0001705, num_neighbors = 3, num_heads=3, num_layers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ff6c8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [09:03<00:00, 18.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.009947594273345975, 0.043934115004317265, 0.012954903382886821) \n",
      " Rec by N latest items (0.00996468309725514, 0.04404577646436295, 0.012783288988530275)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [09:01<00:00, 18.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.01037291611286301, 0.04465013060431617, 0.013723015482117278) \n",
      " Rec by N latest items (0.010323548399347642, 0.044249817833571424, 0.013709238430334465)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [09:04<00:00, 18.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.009894429043406348, 0.04378504413243423, 0.013404707998392324) \n",
      " Rec by N latest items (0.009712148255041896, 0.04301798834873037, 0.013045309557448564)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [09:05<00:00, 18.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.010813428018077057, 0.04704222819672648, 0.013679321209389875) \n",
      " Rec by N latest items (0.01098811377359299, 0.04746504304389698, 0.014567431996900375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [09:04<00:00, 18.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.010612159647591324, 0.04551032710362727, 0.015398602455821631) \n",
      " Rec by N latest items (0.010665324877530953, 0.04522377620428991, 0.015352433978709404)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [09:03<00:00, 18.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping\n",
      "RESULT\n",
      "Epoch 5, Rec by latest item: PR@1: 0.024228154786769453|REC@1: 0.006149617382432653|NDCG@1: 0.00043038519474930065\n",
      "Epoch 5, Rec by 10 latest items: PR@1: 0.0266965404625375|REC@1: 0.0065156225765048755|NDCG@1: 0.0003797516424258535\n",
      "Epoch 5, Rec by latest item: PR@5: 0.016815402726616205|REC@5: 0.019918779398371778|NDCG@5: 0.003818187268652324\n",
      "Epoch 5, Rec by 10 latest items: PR@5: 0.016785022595222143|REC@5: 0.019743063790528103|NDCG@5: 0.003720155397081149\n",
      "Epoch 5, Rec by latest item: PR@10: 0.012600159495689994|REC@10: 0.028425258279355714|NDCG@10: 0.006629287576578591\n",
      "Epoch 5, Rec by 10 latest items: PR@10: 0.012303953214597794|REC@10: 0.02762779818972859|NDCG@10: 0.006413826194229203\n",
      "Epoch 5, Rec by latest item: PR@15: 0.010939378979480371|REC@15: 0.03619559039361133|NDCG@15: 0.009476081148280182\n",
      "Epoch 5, Rec by 10 latest items: PR@15: 0.010891277104773101|REC@15: 0.03543934105503367|NDCG@15: 0.009690581408944481\n",
      "Epoch 5, Rec by latest item: PR@20: 0.010146963885619554|REC@20: 0.04361798455702544|NDCG@20: 0.01257457591710121\n",
      "Epoch 5, Rec by 10 latest items: PR@20: 0.01018493904986214|REC@20: 0.04284938058297495|NDCG@20: 0.012889814643687401\n"
     ]
    }
   ],
   "source": [
    "batch_sampler = sampler_module.ItemToItemBatchSampler(\n",
    "    g, user_ntype, item_ntype, args.batch_size)\n",
    "neighbor_sampler = sampler_module.NeighborSampler(\n",
    "    g, user_ntype, item_ntype, args.random_walk_length,\n",
    "    args.random_walk_restart_prob, args.num_random_walks, args.num_neighbors,\n",
    "    args.num_layers)\n",
    "collator = sampler_module.PinSAGECollator(neighbor_sampler, g, item_ntype, textset)\n",
    "dataloader = DataLoader(\n",
    "    batch_sampler,\n",
    "    collate_fn=collator.collate_train,\n",
    "    num_workers=args.num_workers)\n",
    "dataloader_test = DataLoader(\n",
    "    torch.arange(g.number_of_nodes(item_ntype)),\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=collator.collate_test,\n",
    "    num_workers=args.num_workers)\n",
    "dataloader_it = iter(dataloader)\n",
    "# Model\n",
    "model = PinSAGEModel(g, item_ntype, textset, args.hidden_dims, args.num_layers, args.num_heads).to(device)\n",
    "# Optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "base_ndcg = 0\n",
    "# For each batch of head-tail-negative triplets...\n",
    "for epoch_id in range(args.num_epochs):\n",
    "    model.train()\n",
    "    for batch_id in tqdm.trange(args.batches_per_epoch):\n",
    "        pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "        # Copy to GPU\n",
    "        for i in range(len(blocks)):\n",
    "            blocks[i] = blocks[i].to(device)\n",
    "        pos_graph = pos_graph.to(device)\n",
    "        neg_graph = neg_graph.to(device)\n",
    "        loss = model(pos_graph, neg_graph, blocks).mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    #with open(args.output_model_path, 'wb') as f:\n",
    "        #pickle.dump(model, f)\n",
    "        \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        item_batches = torch.arange(g.number_of_nodes(item_ntype)).split(args.batch_size)\n",
    "        h_item_batches = []\n",
    "        for blocks in dataloader_test:\n",
    "            for i in range(len(blocks)):\n",
    "                blocks[i] = blocks[i].to(device)\n",
    "            h_item_batches.append(model.get_repr(blocks))\n",
    "        h_item = torch.cat(h_item_batches, 0)\n",
    "        metrics = evaluation.evaluate_nn(dataset, h_item, args.k, args.batch_size, args.n_latest_items)\n",
    "        if metrics[0][2] - base_ndcg > -0.001:\n",
    "            old_metrics = metrics\n",
    "            base_ndcg = metrics[0][2]\n",
    "            torch.save(model.state_dict(), args.output_model_path)\n",
    "        \n",
    "            print('Rec by latest item', metrics[0],\n",
    "                  '\\n',\n",
    "                  'Rec by N latest items', metrics[1])\n",
    "        else:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "print('RESULT')\n",
    "for k in [1, 5, 10, 15, 20]:\n",
    "    metrics = evaluation.evaluate_nn(dataset, h_item, k, args.batch_size, args.n_latest_items)\n",
    "    \n",
    "    print(f'Epoch {epoch_id}, Rec by latest item: PR@{k}: {metrics[0][0]}|REC@{k}: {metrics[0][1]}|NDCG@{k}: {metrics[0][2]}')\n",
    "    print(f'Epoch {epoch_id}, Rec by {args.n_latest_items} latest items: PR@{k}: {metrics[1][0]}|REC@{k}: {metrics[1][1]}|NDCG@{k}: {metrics[1][2]}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c039cacc",
   "metadata": {},
   "source": [
    "Amazon Video Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c118666b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/amazon.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    \n",
    "\n",
    "args = TrainArgs(output_model_path='models_attention/model_amazon_optuna', num_epochs=5, \n",
    "                 hidden_dims=64, batches_per_epoch=10000, device='cuda', k=20)\n",
    "\n",
    "\n",
    "g = dataset['train-graph']\n",
    "val_matrix = dataset['val-matrix'].tocsr()\n",
    "test_matrix = dataset['test-matrix'].tocsr()\n",
    "item_texts = dataset['item-texts']\n",
    "user_ntype = dataset['user-type']\n",
    "item_ntype = dataset['item-type']\n",
    "user_to_item_etype = dataset['user-to-item-type']\n",
    "timestamp = dataset['timestamp-edge-column']\n",
    "device = torch.device(args.device)\n",
    "# Assign user and movie IDs and use them as features (to learn an individual trainable\n",
    "# embedding for each entity)\n",
    "g.nodes[user_ntype].data['id'] = torch.arange(g.number_of_nodes(user_ntype))\n",
    "g.nodes[item_ntype].data['id'] = torch.arange(g.number_of_nodes(item_ntype))\n",
    "# Prepare torchtext dataset and vocabulary\n",
    "if item_texts is not None:\n",
    "    fields = {}\n",
    "    examples = []\n",
    "    for key, texts in item_texts.items():\n",
    "        fields[key] = data.Field(include_lengths=True, lower=True, batch_first=True)\n",
    "    for i in range(g.number_of_nodes(item_ntype)):\n",
    "        example = data.Example.fromlist(\n",
    "            [item_texts[key][i] for key in item_texts.keys()],\n",
    "            [(key, fields[key]) for key in item_texts.keys()])\n",
    "        examples.append(example)\n",
    "    textset = data.Dataset(examples, fields)\n",
    "    for key, field in fields.items():\n",
    "        field.build_vocab(getattr(textset, key))\n",
    "        #field.build_vocab(getattr(textset, key), vectors='fasttext.simple.300d')\n",
    "else:\n",
    "    textset = None\n",
    "# Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd134636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-27 15:37:14,371]\u001b[0m A new study created in memory with name: no-name-fbfa7fa5-b993-4fd6-b396-25c493a12642\u001b[0m\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:09<00:00, 40.01it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:09<00:00, 40.05it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:11<00:00, 39.75it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:11<00:00, 39.78it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:02<00:00, 41.23it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:01<00:00, 41.37it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:07<00:00, 40.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:11<00:00, 39.83it/s]\n",
      "\u001b[32m[I 2022-05-27 16:17:14,213]\u001b[0m Trial 0 finished with value: 0.011123306176697123 and parameters: {'n_layers': 1, 'n_heads': 2, 'hidden_dims': 76, 'num_epochs': 8, 'learning_rate_init': 0.0005524848167510423, 'num_neighbors': 8}. Best is trial 0 with value: 0.011123306176697123.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    n_heads = trial.suggest_int('n_heads', 1, 3)\n",
    "    hidden_dims = trial.suggest_int('hidden_dims', 64, 256)\n",
    "    num_epochs = trial.suggest_int('num_epochs', 3, 10)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate_init\", 1e-5, 1e-3)\n",
    "    num_neighbors = trial.suggest_int('num_neighbors', 1, 15)\n",
    "    \n",
    "    batch_sampler = sampler_module.ItemToItemBatchSampler(\n",
    "        g, user_ntype, item_ntype, args.batch_size)\n",
    "    neighbor_sampler = sampler_module.NeighborSampler(\n",
    "        g, user_ntype, item_ntype, args.random_walk_length,\n",
    "        args.random_walk_restart_prob, args.num_random_walks, num_neighbors,\n",
    "        n_layers)\n",
    "    collator = sampler_module.PinSAGECollator(neighbor_sampler, g, item_ntype, textset)\n",
    "    dataloader = DataLoader(\n",
    "        batch_sampler,\n",
    "        collate_fn=collator.collate_train,\n",
    "        num_workers=args.num_workers)\n",
    "    dataloader_test = DataLoader(\n",
    "        torch.arange(g.number_of_nodes(item_ntype)),\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=collator.collate_test,\n",
    "        num_workers=args.num_workers)\n",
    "    dataloader_it = iter(dataloader)\n",
    "    \n",
    "    model = PinSAGEModel(g, item_ntype, textset, hidden_dims, n_layers, n_heads).to(args.device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    layers = []\n",
    "\n",
    "    for epoch_id in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch_id in tqdm.trange(args.batches_per_epoch):\n",
    "            pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "            # Copy to GPU\n",
    "            for i in range(len(blocks)):\n",
    "                blocks[i] = blocks[i].to(device)\n",
    "            pos_graph = pos_graph.to(device)\n",
    "            neg_graph = neg_graph.to(device)\n",
    "\n",
    "            loss = model(pos_graph, neg_graph, blocks).mean()\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            item_batches = torch.arange(g.number_of_nodes(item_ntype)).split(args.batch_size)\n",
    "            h_item_batches = []\n",
    "            for blocks in dataloader_test:\n",
    "                for i in range(len(blocks)):\n",
    "                    blocks[i] = blocks[i].to(device)\n",
    "\n",
    "                h_item_batches.append(model.get_repr(blocks))\n",
    "            h_item = torch.cat(h_item_batches, 0)\n",
    "            metrics = evaluation.evaluate_nn(dataset, h_item, args.k, args.batch_size, args.n_latest_items)\n",
    "            \n",
    "    return metrics[0][2]\n",
    "\n",
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1, timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "43a9e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainArgs(output_model_path='models_attention/model_amazon_optuna', num_epochs=8, \n",
    "                 hidden_dims=76, batches_per_epoch=10000, device='cuda', k=20,\n",
    "                 lr = 0.000552, num_neighbors = 8, num_heads=2, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c666a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:16<00:00, 38.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.00835560174188448, 0.06615705923103982, 0.009057618506539878) \n",
      " Rec by N latest items (0.008704473475851307, 0.06493858757309359, 0.009601862724181334)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:20<00:00, 38.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.008541171813143419, 0.068235198523233, 0.009342833919685127) \n",
      " Rec by N latest items (0.008348178939034136, 0.06418700628322747, 0.008554205180549817)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:04<00:00, 40.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.009018705463183063, 0.06903598863446714, 0.010028264581840023) \n",
      " Rec by N latest items (0.00854612034837699, 0.06440055759647947, 0.009023646637802096)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:04<00:00, 40.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.009770882818685938, 0.07458141564823727, 0.011400505240784736) \n",
      " Rec by N latest items (0.009508610451306658, 0.069985502874879, 0.010655284377570278)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:05<00:00, 40.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.009513558986540244, 0.07465711270621028, 0.012349293204482355) \n",
      " Rec by N latest items (0.010020783847981319, 0.07600981805376356, 0.012280222351615262)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:03<00:00, 41.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.009884699129058098, 0.07675211864808085, 0.012025480040597742) \n",
      " Rec by N latest items (0.010080166270784159, 0.07528500854102727, 0.010411217621072187)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [04:17<00:00, 38.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.00940716547901844, 0.07334511486920466, 0.011033831581580684) \n",
      " Rec by N latest items (0.009117676167854482, 0.0682706855519407, 0.008792502972168287)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [05:10<00:00, 32.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec by latest item (0.0103226444972292, 0.07962849165103553, 0.013726529848540408) \n",
      " Rec by N latest items (0.010268210609659864, 0.07617107080761024, 0.01173753361804386)\n",
      "RESULT\n",
      "Epoch 7, Rec by latest item: PR@1: 0.02093230403800475|REC@1: 0.00875798398135897|NDCG@1: 0.0006927949326999208\n",
      "Epoch 7, Rec by 10 latest items: PR@1: 0.014251781472684086|REC@1: 0.005708347762858308|NDCG@1: 9.897070467141726e-05\n",
      "Epoch 7, Rec by latest item: PR@5: 0.014845605700712252|REC@5: 0.029623199596183883|NDCG@5: 0.0032239283095019436\n",
      "Epoch 7, Rec by 10 latest items: PR@5: 0.013687648456056746|REC@5: 0.025491047522049805|NDCG@5: 0.001742295796727916\n",
      "Epoch 7, Rec by latest item: PR@10: 0.012752375296911705|REC@10: 0.04968138626679048|NDCG@10: 0.0073241993338093395\n",
      "Epoch 7, Rec by 10 latest items: PR@10: 0.012227830562153206|REC@10: 0.04556517258844035|NDCG@10: 0.005062716689409276\n",
      "Epoch 7, Rec by latest item: PR@15: 0.011332145684876959|REC@15: 0.06603788944401971|NDCG@15: 0.010562359022544526\n",
      "Epoch 7, Rec by 10 latest items: PR@15: 0.010969253101081789|REC@15: 0.06098306605270465|NDCG@15: 0.00791573086010927\n",
      "Epoch 7, Rec by latest item: PR@20: 0.0103226444972292|REC@20: 0.07962849165103553|NDCG@20: 0.013726529848540408\n",
      "Epoch 7, Rec by 10 latest items: PR@20: 0.010268210609659864|REC@20: 0.07617107080761024|NDCG@20: 0.01173753361804386\n"
     ]
    }
   ],
   "source": [
    "batch_sampler = sampler_module.ItemToItemBatchSampler(\n",
    "    g, user_ntype, item_ntype, args.batch_size)\n",
    "neighbor_sampler = sampler_module.NeighborSampler(\n",
    "    g, user_ntype, item_ntype, args.random_walk_length,\n",
    "    args.random_walk_restart_prob, args.num_random_walks, args.num_neighbors,\n",
    "    args.num_layers)\n",
    "collator = sampler_module.PinSAGECollator(neighbor_sampler, g, item_ntype, textset)\n",
    "dataloader = DataLoader(\n",
    "    batch_sampler,\n",
    "    collate_fn=collator.collate_train,\n",
    "    num_workers=args.num_workers)\n",
    "dataloader_test = DataLoader(\n",
    "    torch.arange(g.number_of_nodes(item_ntype)),\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=collator.collate_test,\n",
    "    num_workers=args.num_workers)\n",
    "dataloader_it = iter(dataloader)\n",
    "# Model\n",
    "model = PinSAGEModel(g, item_ntype, textset, args.hidden_dims, args.num_layers, args.num_heads).to(device)\n",
    "# Optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "base_ndcg = 0\n",
    "# For each batch of head-tail-negative triplets...\n",
    "for epoch_id in range(args.num_epochs):\n",
    "    model.train()\n",
    "    for batch_id in tqdm.trange(args.batches_per_epoch):\n",
    "        pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "        # Copy to GPU\n",
    "        for i in range(len(blocks)):\n",
    "            blocks[i] = blocks[i].to(device)\n",
    "        pos_graph = pos_graph.to(device)\n",
    "        neg_graph = neg_graph.to(device)\n",
    "        loss = model(pos_graph, neg_graph, blocks).mean()\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    #with open(args.output_model_path, 'wb') as f:\n",
    "        #pickle.dump(model, f)\n",
    "        \n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        item_batches = torch.arange(g.number_of_nodes(item_ntype)).split(args.batch_size)\n",
    "        h_item_batches = []\n",
    "        for blocks in dataloader_test:\n",
    "            for i in range(len(blocks)):\n",
    "                blocks[i] = blocks[i].to(device)\n",
    "            h_item_batches.append(model.get_repr(blocks))\n",
    "        h_item = torch.cat(h_item_batches, 0)\n",
    "        metrics = evaluation.evaluate_nn(dataset, h_item, args.k, args.batch_size, args.n_latest_items)\n",
    "        if metrics[0][2] - base_ndcg > -0.001:\n",
    "            old_metrics = metrics\n",
    "            base_ndcg = metrics[0][2]\n",
    "            torch.save(model.state_dict(), args.output_model_path)\n",
    "        \n",
    "            print('Rec by latest item', metrics[0],\n",
    "                  '\\n',\n",
    "                  'Rec by N latest items', metrics[1])\n",
    "        else:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "\n",
    "print('RESULT')\n",
    "for k in [1, 5, 10, 15, 20]:\n",
    "    metrics = evaluation.evaluate_nn(dataset, h_item, k, args.batch_size, args.n_latest_items)\n",
    "    \n",
    "    print(f'Epoch {epoch_id}, Rec by latest item: PR@{k}: {metrics[0][0]}|REC@{k}: {metrics[0][1]}|NDCG@{k}: {metrics[0][2]}')\n",
    "    print(f'Epoch {epoch_id}, Rec by {args.n_latest_items} latest items: PR@{k}: {metrics[1][0]}|REC@{k}: {metrics[1][1]}|NDCG@{k}: {metrics[1][2]}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48f2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

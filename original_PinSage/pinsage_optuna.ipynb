{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7870818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.legacy import data\n",
    "import dgl\n",
    "import tqdm\n",
    "\n",
    "import layers\n",
    "import sampler as sampler_module\n",
    "import evaluation\n",
    "from model import PinSAGEModel\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123b9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TrainArgs:\n",
    "    output_model_path: str\n",
    "    random_walk_length: int = 2\n",
    "    random_walk_restart_prob: float = 0.5\n",
    "    num_random_walks: int = 10\n",
    "    num_neighbors: int = 5\n",
    "    num_layers: int = 2\n",
    "    num_heads: int = 2\n",
    "    hidden_dims: int = 16\n",
    "    batch_size: int = 64\n",
    "    device: str = 'cpu'\n",
    "    num_epochs: int = 1\n",
    "    batches_per_epoch: int = 20000\n",
    "    num_workers: int = 0\n",
    "    lr: float = 3e-5\n",
    "    k: int = 10\n",
    "    n_latest_items: int = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2555855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainArgs(output_model_path='abc', num_epochs=5, batches_per_epoch=10000, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0bca37",
   "metadata": {},
   "source": [
    "ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea0a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/data_ml.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b079719",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dataset['train-graph']\n",
    "val_matrix = dataset['val-matrix'].tocsr()\n",
    "test_matrix = dataset['test-matrix'].tocsr()\n",
    "item_texts = dataset['item-texts']\n",
    "user_ntype = dataset['user-type']\n",
    "item_ntype = dataset['item-type']\n",
    "user_to_item_etype = dataset['user-to-item-type']\n",
    "timestamp = dataset['timestamp-edge-column']\n",
    "device = torch.device(args.device)\n",
    "# Assign user and movie IDs and use them as features (to learn an individual trainable\n",
    "# embedding for each entity)\n",
    "g.nodes[user_ntype].data['id'] = torch.arange(g.number_of_nodes(user_ntype))\n",
    "g.nodes[item_ntype].data['id'] = torch.arange(g.number_of_nodes(item_ntype))\n",
    "# Prepare torchtext dataset and vocabulary\n",
    "if item_texts is not None:\n",
    "    fields = {}\n",
    "    examples = []\n",
    "    for key, texts in item_texts.items():\n",
    "        fields[key] = data.Field(include_lengths=True, lower=True, batch_first=True)\n",
    "    for i in range(g.number_of_nodes(item_ntype)):\n",
    "        example = data.Example.fromlist(\n",
    "            [item_texts[key][i] for key in item_texts.keys()],\n",
    "            [(key, fields[key]) for key in item_texts.keys()])\n",
    "        examples.append(example)\n",
    "    textset = data.Dataset(examples, fields)\n",
    "    for key, field in fields.items():\n",
    "        field.build_vocab(getattr(textset, key))\n",
    "        #field.build_vocab(getattr(textset, key), vectors='fasttext.simple.300d')\n",
    "else:\n",
    "    textset = None\n",
    "# Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3fe3e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-29 13:47:19,886]\u001b[0m A new study created in memory with name: no-name-8a439f5a-eda2-4fe2-9027-e635a207fa0a\u001b[0m\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:50<00:00, 58.69it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [02:45<00:00, 60.46it/s]\n",
      "\u001b[32m[I 2022-05-29 13:53:22,921]\u001b[0m Trial 0 finished with value: 0.23542091285041467 and parameters: {'n_layers': 1, 'hidden_dims': 50, 'learning_rate_init': 0.0008057495643996801, 'num_neighbors': 6}. Best is trial 0 with value: 0.23542091285041467.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    hidden_dims = trial.suggest_int('hidden_dims', 32, 128)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate_init\", 1e-5, 1e-3)\n",
    "    num_neighbors = trial.suggest_int('num_neighbors', 1, 15)\n",
    "    \n",
    "    batch_sampler = sampler_module.ItemToItemBatchSampler(\n",
    "        g, user_ntype, item_ntype, args.batch_size)\n",
    "    neighbor_sampler = sampler_module.NeighborSampler(\n",
    "        g, user_ntype, item_ntype, args.random_walk_length,\n",
    "        args.random_walk_restart_prob, args.num_random_walks, num_neighbors,\n",
    "        n_layers)\n",
    "    collator = sampler_module.PinSAGECollator(neighbor_sampler, g, item_ntype, textset)\n",
    "    dataloader = DataLoader(\n",
    "        batch_sampler,\n",
    "        collate_fn=collator.collate_train,\n",
    "        num_workers=args.num_workers)\n",
    "    dataloader_test = DataLoader(\n",
    "        torch.arange(g.number_of_nodes(item_ntype)),\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=collator.collate_test,\n",
    "        num_workers=args.num_workers)\n",
    "    dataloader_it = iter(dataloader)\n",
    "    \n",
    "    model = PinSAGEModel(g, item_ntype, textset, hidden_dims, n_layers).to(args.device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    layers = []\n",
    "\n",
    "    for epoch_id in range(2):\n",
    "        model.train()\n",
    "        for batch_id in tqdm.trange(args.batches_per_epoch):\n",
    "            pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "            # Copy to GPU\n",
    "            for i in range(len(blocks)):\n",
    "                blocks[i] = blocks[i].to(device)\n",
    "            pos_graph = pos_graph.to(device)\n",
    "            neg_graph = neg_graph.to(device)\n",
    "\n",
    "            loss = model(pos_graph, neg_graph, blocks).mean()\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            item_batches = torch.arange(g.number_of_nodes(item_ntype)).split(args.batch_size)\n",
    "            h_item_batches = []\n",
    "            for blocks in dataloader_test:\n",
    "                for i in range(len(blocks)):\n",
    "                    blocks[i] = blocks[i].to(device)\n",
    "\n",
    "                h_item_batches.append(model.get_repr(blocks))\n",
    "            h_item = torch.cat(h_item_batches, 0)\n",
    "            metrics = evaluation.evaluate_nn(dataset, h_item, args.k, args.batch_size, args.n_latest_items)\n",
    "            \n",
    "    return metrics[0][2]\n",
    "\n",
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1, timeout=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621aba00",
   "metadata": {},
   "source": [
    "Ta Feng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2958b9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/tafeng.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    \n",
    "g = dataset['train-graph']\n",
    "val_matrix = dataset['val-matrix'].tocsr()\n",
    "test_matrix = dataset['test-matrix'].tocsr()\n",
    "item_texts = dataset['item-texts']\n",
    "user_ntype = dataset['user-type']\n",
    "item_ntype = dataset['item-type']\n",
    "user_to_item_etype = dataset['user-to-item-type']\n",
    "timestamp = dataset['timestamp-edge-column']\n",
    "device = torch.device(args.device)\n",
    "# Assign user and movie IDs and use them as features (to learn an individual trainable\n",
    "# embedding for each entity)\n",
    "g.nodes[user_ntype].data['id'] = torch.arange(g.number_of_nodes(user_ntype))\n",
    "g.nodes[item_ntype].data['id'] = torch.arange(g.number_of_nodes(item_ntype))\n",
    "# Prepare torchtext dataset and vocabulary\n",
    "if item_texts is not None:\n",
    "    fields = {}\n",
    "    examples = []\n",
    "    for key, texts in item_texts.items():\n",
    "        fields[key] = data.Field(include_lengths=True, lower=True, batch_first=True)\n",
    "    for i in range(g.number_of_nodes(item_ntype)):\n",
    "        example = data.Example.fromlist(\n",
    "            [item_texts[key][i] for key in item_texts.keys()],\n",
    "            [(key, fields[key]) for key in item_texts.keys()])\n",
    "        examples.append(example)\n",
    "    textset = data.Dataset(examples, fields)\n",
    "    for key, field in fields.items():\n",
    "        field.build_vocab(getattr(textset, key))\n",
    "        #field.build_vocab(getattr(textset, key), vectors='fasttext.simple.300d')\n",
    "else:\n",
    "    textset = None\n",
    "# Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0baa68d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-29 13:53:31,444]\u001b[0m A new study created in memory with name: no-name-09121703-9026-4cf0-96a3-4593680c7dac\u001b[0m\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [09:59<00:00, 16.67it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [10:01<00:00, 16.64it/s]\n",
      "\u001b[32m[I 2022-05-29 14:17:49,076]\u001b[0m Trial 0 finished with value: 0.005602185466707845 and parameters: {'n_layers': 3, 'hidden_dims': 68, 'learning_rate_init': 0.0001351776812325878, 'num_neighbors': 6}. Best is trial 0 with value: 0.005602185466707845.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    hidden_dims = trial.suggest_int('hidden_dims', 16, 128)\n",
    "    #num_epochs = trial.suggest_int('num_epochs', 3, 10)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate_init\", 1e-5, 1e-3)\n",
    "    num_neighbors = trial.suggest_int('num_neighbors', 1, 15)\n",
    "    \n",
    "    batch_sampler = sampler_module.ItemToItemBatchSampler(\n",
    "        g, user_ntype, item_ntype, args.batch_size)\n",
    "    neighbor_sampler = sampler_module.NeighborSampler(\n",
    "        g, user_ntype, item_ntype, args.random_walk_length,\n",
    "        args.random_walk_restart_prob, args.num_random_walks, num_neighbors,\n",
    "        n_layers)\n",
    "    collator = sampler_module.PinSAGECollator(neighbor_sampler, g, item_ntype, textset)\n",
    "    dataloader = DataLoader(\n",
    "        batch_sampler,\n",
    "        collate_fn=collator.collate_train,\n",
    "        num_workers=args.num_workers)\n",
    "    dataloader_test = DataLoader(\n",
    "        torch.arange(g.number_of_nodes(item_ntype)),\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=collator.collate_test,\n",
    "        num_workers=args.num_workers)\n",
    "    dataloader_it = iter(dataloader)\n",
    "    \n",
    "    model = PinSAGEModel(g, item_ntype, textset, hidden_dims, n_layers).to(args.device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    layers = []\n",
    "\n",
    "    for epoch_id in range(2):\n",
    "        model.train()\n",
    "        for batch_id in tqdm.trange(args.batches_per_epoch):\n",
    "            pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "            # Copy to GPU\n",
    "            for i in range(len(blocks)):\n",
    "                blocks[i] = blocks[i].to(device)\n",
    "            pos_graph = pos_graph.to(device)\n",
    "            neg_graph = neg_graph.to(device)\n",
    "\n",
    "            loss = model(pos_graph, neg_graph, blocks).mean()\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            item_batches = torch.arange(g.number_of_nodes(item_ntype)).split(args.batch_size)\n",
    "            h_item_batches = []\n",
    "            for blocks in dataloader_test:\n",
    "                for i in range(len(blocks)):\n",
    "                    blocks[i] = blocks[i].to(device)\n",
    "\n",
    "                h_item_batches.append(model.get_repr(blocks))\n",
    "            h_item = torch.cat(h_item_batches, 0)\n",
    "            metrics = evaluation.evaluate_nn(dataset, h_item, args.k, args.batch_size, args.n_latest_items)\n",
    "            \n",
    "    return metrics[0][2]\n",
    "\n",
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1, timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3596529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa3eae38",
   "metadata": {},
   "source": [
    "Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00e35cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/amazon.pkl', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    \n",
    "g = dataset['train-graph']\n",
    "val_matrix = dataset['val-matrix'].tocsr()\n",
    "test_matrix = dataset['test-matrix'].tocsr()\n",
    "item_texts = dataset['item-texts']\n",
    "user_ntype = dataset['user-type']\n",
    "item_ntype = dataset['item-type']\n",
    "user_to_item_etype = dataset['user-to-item-type']\n",
    "timestamp = dataset['timestamp-edge-column']\n",
    "device = torch.device(args.device)\n",
    "# Assign user and movie IDs and use them as features (to learn an individual trainable\n",
    "# embedding for each entity)\n",
    "g.nodes[user_ntype].data['id'] = torch.arange(g.number_of_nodes(user_ntype))\n",
    "g.nodes[item_ntype].data['id'] = torch.arange(g.number_of_nodes(item_ntype))\n",
    "# Prepare torchtext dataset and vocabulary\n",
    "if item_texts is not None:\n",
    "    fields = {}\n",
    "    examples = []\n",
    "    for key, texts in item_texts.items():\n",
    "        fields[key] = data.Field(include_lengths=True, lower=True, batch_first=True)\n",
    "    for i in range(g.number_of_nodes(item_ntype)):\n",
    "        example = data.Example.fromlist(\n",
    "            [item_texts[key][i] for key in item_texts.keys()],\n",
    "            [(key, fields[key]) for key in item_texts.keys()])\n",
    "        examples.append(example)\n",
    "    textset = data.Dataset(examples, fields)\n",
    "    for key, field in fields.items():\n",
    "        field.build_vocab(getattr(textset, key))\n",
    "        #field.build_vocab(getattr(textset, key), vectors='fasttext.simple.300d')\n",
    "else:\n",
    "    textset = None\n",
    "# Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e2e9f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-05-29 14:18:33,209]\u001b[0m A new study created in memory with name: no-name-43bfc550-c131-4a93-a5a0-633d35ca9ab3\u001b[0m\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [15:54<00:00, 10.47it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [18:07<00:00,  9.20it/s]\n",
      "\u001b[32m[I 2022-05-29 14:56:07,691]\u001b[0m Trial 0 finished with value: 0.004244697693350659 and parameters: {'n_layers': 3, 'hidden_dims': 131, 'learning_rate_init': 0.0006658629757644586, 'num_neighbors': 11}. Best is trial 0 with value: 0.004244697693350659.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    # 2. Suggest values of the hyperparameters using a trial object.\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 4)\n",
    "    hidden_dims = trial.suggest_int('hidden_dims', 64, 256)\n",
    "    #num_epochs = trial.suggest_int('num_epochs', 3, 15)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate_init\", 1e-5, 1e-3)\n",
    "    num_neighbors = trial.suggest_int('num_neighbors', 1, 15)\n",
    "    \n",
    "    batch_sampler = sampler_module.ItemToItemBatchSampler(\n",
    "        g, user_ntype, item_ntype, args.batch_size)\n",
    "    neighbor_sampler = sampler_module.NeighborSampler(\n",
    "        g, user_ntype, item_ntype, args.random_walk_length,\n",
    "        args.random_walk_restart_prob, args.num_random_walks, num_neighbors,\n",
    "        n_layers)\n",
    "    collator = sampler_module.PinSAGECollator(neighbor_sampler, g, item_ntype, textset)\n",
    "    dataloader = DataLoader(\n",
    "        batch_sampler,\n",
    "        collate_fn=collator.collate_train,\n",
    "        num_workers=args.num_workers)\n",
    "    dataloader_test = DataLoader(\n",
    "        torch.arange(g.number_of_nodes(item_ntype)),\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=collator.collate_test,\n",
    "        num_workers=args.num_workers)\n",
    "    dataloader_it = iter(dataloader)\n",
    "    \n",
    "    model = PinSAGEModel(g, item_ntype, textset, hidden_dims, n_layers).to(args.device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    layers = []\n",
    "\n",
    "    for epoch_id in range(2):\n",
    "        model.train()\n",
    "        for batch_id in tqdm.trange(args.batches_per_epoch):\n",
    "            pos_graph, neg_graph, blocks = next(dataloader_it)\n",
    "            # Copy to GPU\n",
    "            for i in range(len(blocks)):\n",
    "                blocks[i] = blocks[i].to(device)\n",
    "            pos_graph = pos_graph.to(device)\n",
    "            neg_graph = neg_graph.to(device)\n",
    "\n",
    "            loss = model(pos_graph, neg_graph, blocks).mean()\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            item_batches = torch.arange(g.number_of_nodes(item_ntype)).split(args.batch_size)\n",
    "            h_item_batches = []\n",
    "            for blocks in dataloader_test:\n",
    "                for i in range(len(blocks)):\n",
    "                    blocks[i] = blocks[i].to(device)\n",
    "\n",
    "                h_item_batches.append(model.get_repr(blocks))\n",
    "            h_item = torch.cat(h_item_batches, 0)\n",
    "            metrics = evaluation.evaluate_nn(dataset, h_item, args.k, args.batch_size, args.n_latest_items)\n",
    "            \n",
    "    return metrics[0][2]\n",
    "\n",
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1, timeout=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36799213",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
